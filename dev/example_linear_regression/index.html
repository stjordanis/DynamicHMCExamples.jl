<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear regression · DynamicHMCExamples.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>DynamicHMCExamples.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Overview</a></li><li><a class="toctext" href="../example_independent_bernoulli/">Estimate Bernoulli draws probabilility</a></li><li class="current"><a class="toctext" href>Linear regression</a><ul class="internal"></ul></li><li><a class="toctext" href="../example_logistic_regression/">Logistic regression</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Linear regression</a></li></ul><a class="edit-page" href="https://github.com/tpapp/DynamicHMCExamples.jl/blob/master/src/example_linear_regression.jl"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Linear regression</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Linear-regression-1" href="#Linear-regression-1">Linear regression</a></h1><p>We estimate simple linear regression model with a half-T prior. First, we load the packages we use.</p><pre><code class="language-julia">using TransformVariables, LogDensityProblems, DynamicHMC, DynamicHMC.Diagnostics
using MCMCDiagnostics
using Parameters, Statistics, Random, Distributions
import ForwardDiff              # use for AD</code></pre><p>Then define a structure to hold the data: observables, covariates, and the degrees of freedom for the prior.</p><pre><code class="language-julia">&quot;&quot;&quot;
Linear regression model ``y ∼ Xβ + ϵ``, where ``ϵ ∼ N(0, σ²)`` IID.

Flat prior for `β`, half-T for `σ`.
&quot;&quot;&quot;
struct LinearRegressionProblem{TY &lt;: AbstractVector, TX &lt;: AbstractMatrix,
                               Tν &lt;: Real}
    &quot;Observations.&quot;
    y::TY
    &quot;Covariates&quot;
    X::TX
    &quot;Degrees of freedom for prior.&quot;
    ν::Tν
end</code></pre><pre><code class="language-none">Main.ex-example_linear_regression.LinearRegressionProblem</code></pre><p>Then make the type callable with the parameters <em>as a single argument</em>.</p><pre><code class="language-julia">function (problem::LinearRegressionProblem)(θ)
    @unpack y, X, ν = problem   # extract the data
    @unpack β, σ = θ            # works on the named tuple too
    loglikelihood(Normal(0, σ), y .- X*β) + logpdf(TDist(ν), σ)
end</code></pre><p>We should test this, also, this would be a good place to benchmark and optimize more complicated problems.</p><pre><code class="language-julia">N = 100
X = hcat(ones(N), randn(N, 2));
β = [1.0, 2.0, -1.0]
σ = 0.5
y = X*β .+ randn(N) .* σ;
p = LinearRegressionProblem(y, X, 1.0);
p((β = β, σ = σ))</code></pre><pre><code class="language-none">-65.63183104613307</code></pre><p>For this problem, we write a function to return the transformation (as it varies with the number of covariates).</p><pre><code class="language-julia">function problem_transformation(p::LinearRegressionProblem)
    as((β = as(Array, size(p.X, 2)), σ = asℝ₊))
end</code></pre><pre><code class="language-none">problem_transformation (generic function with 1 method)</code></pre><p>Wrap the problem with a transformation, then use ForwardDiff for the gradient.</p><pre><code class="language-julia">t = problem_transformation(p)
P = TransformedLogDensity(t, p)
∇P = ADgradient(:ForwardDiff, P);</code></pre><pre><code class="language-none">ForwardDiff AD wrapper for TransformedLogDensity of dimension 4, w/ chunk size 4</code></pre><p>Finally, we sample from the posterior. <code>chain</code> holds the chain (positions and diagnostic information), while the second returned value is the tuned sampler which would allow continuation of sampling.</p><pre><code class="language-julia">results = mcmc_with_warmup(Random.GLOBAL_RNG, ∇P, 1000);</code></pre><pre><code class="language-none">(chain = Array{Float64,1}[[0.9777215419144598, 1.8617695315305556, -0.9869781929503728, -0.8160985940833319], [0.9648229994219905, 1.891446705828946, -0.9767269843469037, -0.8493284686343529], [0.9895020430007456, 1.9463060107569525, -0.9668199723290178, -0.6914825377683114], [0.9867091846676462, 1.9217571807413836, -0.9953698214581957, -0.8206242572354204], [0.9587066113806241, 1.9286606447593189, -0.9827586646973497, -0.8324981103441805], [0.9862965114141599, 1.973585565210892, -1.0269368041302358, -0.7803456895059551], [1.0605019278721362, 1.9360103733140643, -1.0662905353254155, -0.6892309380575138], [1.03295184554052, 1.9118802656818967, -1.0476434561416303, -0.7026572359867347], [0.9589397533210174, 1.9516360346615396, -0.9364226451485954, -0.8299157536817723], [0.9117728207965194, 1.93580628941436, -0.9643883500642055, -0.8602029070907411]  …  [1.0540204676660256, 1.9212441626171064, -1.032487159263248, -0.722466478374336], [0.9628583789053545, 1.949860004185576, -0.9131792178568146, -0.7698153771114613], [0.9669515607684357, 1.9308363062928868, -0.914715977135519, -0.767093335925176], [0.9367234868595891, 1.9031567603559323, -0.9792741970319468, -0.7547212330301195], [0.9720779356184739, 1.9779948199990602, -1.0098795124543827, -0.7999318425770521], [0.9728473221467531, 1.9767483957472267, -1.0304732995703683, -0.8549590222187303], [1.0182357084046008, 1.9676368470001413, -0.9978970678089562, -0.7010865645083277], [1.0531993400763757, 2.0073943877352645, -1.058153923959006, -0.7544411718840317], [0.990655784114077, 1.953808894573023, -1.0476389010090614, -0.7428755293160662], [1.0156882501917404, 1.9565332813553644, -0.981365852618568, -0.6874836517582784]], tree_statistics = DynamicHMC.TreeStatisticsNUTS[DynamicHMC.TreeStatisticsNUTS(-65.75205074934674, 2, turning at positions -1:2, 0.9452129223465517, 3, DynamicHMC.Directions(0x358cf1ea)), DynamicHMC.TreeStatisticsNUTS(-66.92500964545005, 4, turning at positions 14:17, 0.9387833439984874, 19, DynamicHMC.Directions(0x55624dbd)), DynamicHMC.TreeStatisticsNUTS(-66.11855201907305, 2, turning at positions 0:3, 0.9477082876052809, 3, DynamicHMC.Directions(0x6c57b41f)), DynamicHMC.TreeStatisticsNUTS(-66.74474211320866, 3, turning at positions -2:-5, 0.9173245578891126, 11, DynamicHMC.Directions(0x0233f266)), DynamicHMC.TreeStatisticsNUTS(-65.48933453736859, 3, turning at positions -1:6, 0.9090666074095132, 7, DynamicHMC.Directions(0x4187edce)), DynamicHMC.TreeStatisticsNUTS(-67.1532124623594, 2, turning at positions -1:2, 0.8255018848010874, 3, DynamicHMC.Directions(0xf1993532)), DynamicHMC.TreeStatisticsNUTS(-68.33221291461372, 2, turning at positions 2:5, 0.8788630452631977, 7, DynamicHMC.Directions(0x56e2185d)), DynamicHMC.TreeStatisticsNUTS(-68.0537951013936, 2, turning at positions -2:1, 0.9999999999999999, 3, DynamicHMC.Directions(0xac417459)), DynamicHMC.TreeStatisticsNUTS(-67.7305253037606, 3, turning at positions -5:2, 0.9702119883600152, 7, DynamicHMC.Directions(0x410d6b02)), DynamicHMC.TreeStatisticsNUTS(-67.3989797352438, 2, turning at positions -1:2, 0.8625327117795313, 3, DynamicHMC.Directions(0x10754412))  …  DynamicHMC.TreeStatisticsNUTS(-70.27220642655567, 2, turning at positions -2:-5, 0.9470648083324156, 7, DynamicHMC.Directions(0x62cbb8f2)), DynamicHMC.TreeStatisticsNUTS(-69.47959374876751, 3, turning at positions -1:6, 0.83835856686521, 7, DynamicHMC.Directions(0xfd10a6b6)), DynamicHMC.TreeStatisticsNUTS(-66.82058154185279, 3, turning at positions 0:7, 0.9832419245083871, 7, DynamicHMC.Directions(0x9e9d495f)), DynamicHMC.TreeStatisticsNUTS(-66.38823811268166, 3, turning at positions -2:5, 0.9939789662919015, 7, DynamicHMC.Directions(0xdd3ecbcd)), DynamicHMC.TreeStatisticsNUTS(-66.37966888785033, 3, turning at positions -6:1, 0.9490332904940197, 7, DynamicHMC.Directions(0x84a8cf01)), DynamicHMC.TreeStatisticsNUTS(-66.54164409547263, 2, turning at positions -1:2, 0.9260844179585735, 3, DynamicHMC.Directions(0xd3293c3e)), DynamicHMC.TreeStatisticsNUTS(-68.419501124978, 3, turning at positions -5:-8, 0.9439617004838216, 15, DynamicHMC.Directions(0x6bc28e87)), DynamicHMC.TreeStatisticsNUTS(-69.75879792104264, 2, turning at positions -1:2, 0.8258847254536507, 3, DynamicHMC.Directions(0x4f5b4abe)), DynamicHMC.TreeStatisticsNUTS(-68.35382078505918, 2, turning at positions -3:0, 0.9999999999999999, 3, DynamicHMC.Directions(0x8378ff08)), DynamicHMC.TreeStatisticsNUTS(-66.97169856653149, 2, turning at positions -3:-6, 0.9654309268538068, 7, DynamicHMC.Directions(0x5ea0bdc1))], κ = Gaussian kinetic energy (LinearAlgebra.Diagonal), √diag(M⁻¹): [0.0460104092123736, 0.050628711176072816, 0.03791596782672462, 0.07552295334225766], ϵ = 0.7803943232793339)</code></pre><p>We use the transformation to obtain the posterior from the chain.</p><pre><code class="language-julia">posterior = transform.(t, results.chain);</code></pre><pre><code class="language-none">1000-element Array{NamedTuple{(:β, :σ),Tuple{Array{Float64,1},Float64}},1}:
 (β = [0.9777215419144598, 1.8617695315305556, -0.9869781929503728], σ = 0.44215331343015774)
 (β = [0.9648229994219905, 1.891446705828946, -0.9767269843469037], σ = 0.4277020508755689)  
 (β = [0.9895020430007456, 1.9463060107569525, -0.9668199723290178], σ = 0.5008330145392816) 
 (β = [0.9867091846676462, 1.9217571807413836, -0.9953698214581957], σ = 0.44015679765857946)
 (β = [0.9587066113806241, 1.9286606447593189, -0.9827586646973497], σ = 0.434961346551379)  
 (β = [0.9862965114141599, 1.973585565210892, -1.0269368041302358], σ = 0.45824757254453485) 
 (β = [1.0605019278721362, 1.9360103733140643, -1.0662905353254155], σ = 0.5019619605002242) 
 (β = [1.03295184554052, 1.9118802656818967, -1.0476434561416303], σ = 0.4952675110702128)   
 (β = [0.9589397533210174, 1.9516360346615396, -0.9364226451485954], σ = 0.4360860234159347) 
 (β = [0.9117728207965194, 1.93580628941436, -0.9643883500642055], σ = 0.42307622844118853)  
 ⋮                                                                                           
 (β = [0.9628583789053545, 1.949860004185576, -0.9131792178568146], σ = 0.4630985590128582)  
 (β = [0.9669515607684357, 1.9308363062928868, -0.914715977135519], σ = 0.464360849587855)   
 (β = [0.9367234868595891, 1.9031567603559323, -0.9792741970319468], σ = 0.47014165642961814)
 (β = [0.9720779356184739, 1.9779948199990602, -1.0098795124543827], σ = 0.4493595902651595) 
 (β = [0.9728473221467531, 1.9767483957472267, -1.0304732995703683], σ = 0.42530061860092405)
 (β = [1.0182357084046008, 1.9676368470001413, -0.9978970678089562], σ = 0.4960460248586839) 
 (β = [1.0531993400763757, 2.0073943877352645, -1.058153923959006], σ = 0.47027334328006587) 
 (β = [0.990655784114077, 1.953808894573023, -1.0476389010090614], σ = 0.47574393112898694)  
 (β = [1.0156882501917404, 1.9565332813553644, -0.981365852618568], σ = 0.5028397984503179)  </code></pre><p>Extract the parameter posterior means: <code>β</code>,</p><pre><code class="language-julia">posterior_β = mean(first, posterior)</code></pre><pre><code class="language-none">3-element Array{Float64,1}:
  0.9859167716022955
  1.9314922071548435
 -0.9834096540394556</code></pre><p>then <code>σ</code>:</p><pre><code class="language-julia">posterior_σ = mean(last, posterior)</code></pre><pre><code class="language-none">0.46421140905493624</code></pre><p>Effective sample sizes (of untransformed draws)</p><pre><code class="language-julia">ess = vec(mapslices(effective_sample_size,
                    DynamicHMC.position_matrix(results.chain);
                    dims = 2))</code></pre><pre><code class="language-none">4-element Array{Float64,1}:
  732.4402649394128
  883.9905916430249
  789.0492478779876
 1000.0            </code></pre><p>NUTS-specific statistics</p><pre><code class="language-julia">summarize_tree_statistics(results.tree_statistics)</code></pre><pre><code class="language-none">Hamiltonian Monte Carlo sample of length 1000
  acceptance rate mean: 0.9, 5/25/50/75/95%: 0.68 0.85 0.94 0.98 1.0
  termination: divergence =&gt; 0%, max_depth =&gt; 0%, turning =&gt; 100%
  depth: 0 =&gt; 0%, 1 =&gt; 2%, 2 =&gt; 55%, 3 =&gt; 35%, 4 =&gt; 6%, 5 =&gt; 2%, 6 =&gt; 1%</code></pre><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p><footer><hr/><a class="previous" href="../example_independent_bernoulli/"><span class="direction">Previous</span><span class="title">Estimate Bernoulli draws probabilility</span></a><a class="next" href="../example_logistic_regression/"><span class="direction">Next</span><span class="title">Logistic regression</span></a></footer></article></body></html>
